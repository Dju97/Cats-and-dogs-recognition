{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CatsAndDogsRecognition.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Dju97/Cats-and-dogs-recognition/blob/master/CatsAndDogsRecognition.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "pLXuRLsUf2if",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Recognizing dogs and cats\n",
        "\n",
        "The purpose of this project is to build a first end to end reflex-based AI model to teach computers to [**understand images**](https://www.ted.com/talks/fei_fei_li_how_we_re_teaching_computers_to_understand_pictures).\n",
        "\n",
        "In particular, the objective of this project is to write an AI application able to recognize cats and dogs on images. My application take an image as input and is able to say wheter the image contains a dog or a cat. I worked with the data of the [**Dogs vs Cats**](https://www.kaggle.com/c/dogs-vs-cats) competition from Kaggle. This competition was launched in 2013 and the first place was obtained by [Pierre Sermanet](https://research.google.com/pubs/PierreSermanet.html), actually Research Scientist at Google Brain, by using the [Overfeat](http://cilvr.nyu.edu/doku.php?id=software:overfeat:start#overfeatobject_recognizer_feature_extractor) deep learning library he wrote during his PhD at New York University under the supervision of [Yann Le Cun](http://yann.lecun.com/), Director of AI Research at Facebook. He obtained $1.09%$ of classification errors.\n",
        "\n",
        "Two approaches has been used to adress this problem :\n",
        "1. A traditional pattern recognition model in which hand-crafted features are extracted from images and used to represent them and to train classifiers. This was just for testing since I know it's less performing than CNN, thus I haven't past much time to optimize these methods.\n",
        "2. A modern representation learning approach in which deep convolutional neural networks (CNN) are used to learn the image representations.\n",
        "\n",
        "    "
      ]
    },
    {
      "metadata": {
        "collapsed": true,
        "id": "dH8ReFJ_f2ii",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Part 0 : Requirements\n",
        "A set of packages will be useful to handle the first part of this study case.\n",
        "\n",
        "    pip3 install -r requirements.txt"
      ]
    },
    {
      "metadata": {
        "id": "kGUgwllWf2ii",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Part 1 : A small introduction to image classification\n",
        "\n",
        "In this section, I introduced the image classification problem which consists in assigning to an input image one label from a fixed set of labels and which is one of the big challenge of computer vision and artifial intelligence. In our case, we will only consider two labels $\\{dog, cat\\}$. This small part also aims at familiarizing me with computer vision librairies that I used in this course :\n",
        "+ OpenCV : [http://opencv.org/](http://opencv.org/) or Scikit-image ([http://scikit-image.org/](http://scikit-image.org/)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "Ga8pb6A0f2ik",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### A simple image classification pipeline\n",
        "\n",
        "To built my image classification algorithm, I have followed the principle of a machine learning approach for image classification which consists in :\n",
        "1. Collecting and preparing a dataset of images and their corresponding labels.\n",
        "2. Using a machine learning algorithm to train a classifier.\n",
        "3. Evaluate the classifier on new images.\n"
      ]
    },
    {
      "metadata": {
        "id": "nefStzrDf2ik",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### Having a look on the available data\n",
        "\n",
        "First, I've have downloaded the dataset that has been used to train and test my model. You'll find the dataset in the data folder.This dataset contains 25,000 labelled dog and cat photos available for training, and 12,500 in the test set that we have to try to label for the Kaggle competition.\n",
        "\n",
        "As usual, the standard practice in machine learning is to split the available data into three different subsets :\n",
        "+ The **training set** : to learn the model.\n",
        "+ The **validation set** : to fine-tune the parameters of the model\n",
        "+ The **testing set** : to evaluate the learned model.\n",
        "\n",
        "In our case, the preparation of the data has been done and that the test and train sets are in separate subdirectories in which data for each category (cats and dogs) is also into subdirectories. Nevertheless, there is no validation set and I then had to build one myself.\n",
        "\n",
        "The archive also contains a directory named sampleDeep I've generated taking random pictures from the training data set. Since training and validating the entire dataset can take some time, it is a good practice to run algorithm on a small sample of your training and validation data before to run it on the entire set of data.\n"
      ]
    },
    {
      "metadata": {
        "id": "erWWgHKOf2il",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##### Image representation\n",
        "\n",
        "I first wanted to try methods without using deep learning and using older algorithms. I then tried task to built a representation of the data, i.e. a feature vector which values quantify the contents of the image. I then searched documentation on what could be meaningful. Here, I just represented the images by two alternative representations to compare them.\n",
        "+ A first representation is built using the raw data by simply resizing an input image to a fixed size (here $32 \\times 32$ pixels)and then by flattening the RBG pixel intensities into a single vectors of numbers.\n",
        "+ A second representation is built from the color histogram that characterizes the color distribution of the image. For this representation a color conversion into the HSV color space could be useful.\n",
        "    "
      ]
    },
    {
      "metadata": {
        "id": "wuJwlVt2f2im",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from skimage import data,transform,io,color\n",
        "import pandas as pd\n",
        "\n",
        "def image_to_feature_vector(image, size=(32, 32)):\n",
        "    # resize the image to a fixed size, then flatten the image into a list of raw pixel intensities\n",
        "    res  = transform.resize(image,size)\n",
        "    res = res.flatten()\n",
        "    return list(res)\n",
        "\n",
        "\n",
        "def build_HSV_color_histogram_vector(image,bins=(8, 8, 8)):\n",
        "    # extract a 3D color histogram from the HSV color space using the supplied number of `bins` per channel and return it as a feature vector\n",
        "    res = color.convert_colorspace(image,'RGB','HSV')\n",
        "    results = []\n",
        "    for i,bin1 in enumerate(bins):\n",
        "        results.extend(np.histogram(res[:,:,i],bin1)[0])\n",
        "    return results"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "S9l8YQRSf2ir",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### Dataset prepatation and feature extraction\n",
        "The dataset has to be prepared for feature extraction :\n",
        "+ Three lists will be initialized to store the raw pixel representation, the color distribution (histogram) representation and the class labels themselves.\n",
        "+ Then, the lists will be completed by extracted the features from the dataset (don't forgot to begin by testing your program on the sample dataset before to apply it on the whole dataset).\n",
        "\n",
        "For this step, we can used the paths.py file [here](https://github.com/jrosebr1/imutils/blob/master/imutils/paths.py) from imutils\n",
        "\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "EAhDgS76f2is",
        "colab_type": "code",
        "outputId": "79ecb959-1e15-44ce-f591-74a5fb33f1f6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        }
      },
      "cell_type": "code",
      "source": [
        "from imutils import paths\n",
        "from random import shuffle\n",
        "print(\"Describing images...\")\n",
        "imagePaths = list(paths.list_images('./sampleDeep/train'))\n",
        "shuffle(imagePaths)\n",
        "\n",
        "# initialize the raw pixel intensities matrix, the features matrix and labels list\n",
        "rawImages_features = []\n",
        "histogram_features = []\n",
        "labels = []\n",
        "# we take only 200 images to test our representations\n",
        "for imagePath in imagePaths[:300]:\n",
        "    rawImages_features.append(image_to_feature_vector(io.imread(imagePath)))\n",
        "    histogram_features.append(build_HSV_color_histogram_vector(io.imread(imagePath)))\n",
        "    if 'cats' in imagePath:\n",
        "        labels.append(0)\n",
        "    else:\n",
        "        labels.append(1)\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Describing images...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/skimage/transform/_warps.py:84: UserWarning: The default mode, 'constant', will be changed to 'reflect' in skimage 0.15.\n",
            "  warn(\"The default mode, 'constant', will be changed to 'reflect' in \"\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "oYttmSJMf2iw",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### Dataset splitting into training and validation dataset\n",
        "\n",
        "The last step will be to split the training dataset into training dataset and validation dataset."
      ]
    },
    {
      "metadata": {
        "id": "qzQgn0CTf2ix",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "# TO COMPLETE\n",
        "Raw_train,Raw_test,Hist_train,Hist_test,y_train,y_test =  train_test_split(rawImages_features,histogram_features,labels)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "O2cYbDYrf2i1",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### Classification using the K-Nearest Neighbor (KNN) classifier\n",
        "\n",
        "\n",
        "In order to build this simple image classification pipeline, I tried using the k-Nearest Neighbor (KNN) classifier which is a very simple machine learning/image classification approach, even if it's rarely used in practice, its simplicity has enabled me to get quickly an idea of the image classification pipeline. I then firstly tried to build a very simple model, without tuning hyper parameters and using the euclidian metric for simplicity :\n",
        "\n",
        "$d(p,q)=\\sqrt{\\sum_{i=1}^{N}(q_i-p_i)^2}$\n"
      ]
    },
    {
      "metadata": {
        "id": "JS3zg_PGf2i2",
        "colab_type": "code",
        "outputId": "5928e07e-2278-48fb-a390-dec84cc77fb9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "cell_type": "code",
      "source": [
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "\n",
        "# training and evaluation of a k-NN classifer on the raw pixel representation\n",
        "neigh = KNeighborsClassifier(n_neighbors = 10)\n",
        "neigh.fit(Raw_train,y_train)\n",
        "print(\"Raw pixel representation accuracy:\")\n",
        "print(neigh.score(Raw_train,y_train))\n",
        "# training and evaluation of  a k-NN classifer on the color histogram representation\n",
        "neigh = KNeighborsClassifier(n_neighbors = 10)\n",
        "neigh.fit(Hist_train,y_train)\n",
        "print(\"Color histogram representation accuracy:\")\n",
        "print(neigh.score(Hist_train,y_train))\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Raw pixel representation accuracy:\n",
            "0.5911111111111111\n",
            "Color histogram representation accuracy:\n",
            "0.64\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "EvbaSFjzf2i6",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**I then obtained the following scores without tuning hyperparameters**\n",
        "\n",
        "1.   For raw data representation : 0.59\n",
        "2.   For historigram : 0.64\n",
        "\n",
        "These results are a little better than random prediction, and are quite good since the simplicity of the data representations and the model.\n",
        "\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "sD56tfP3f2jA",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### Cross-validation\n",
        "\n",
        "I've used cross-validation to hypertune parameters, in particularly the number of neighbors optimal, searching in a range from 1 to 29."
      ]
    },
    {
      "metadata": {
        "id": "jh9SbBHLf2jB",
        "colab_type": "code",
        "outputId": "92d49958-8682-4b3c-f73c-8d366697a989",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 807
        }
      },
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import cross_val_score\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "# creating odd list of K for KNN\n",
        "params = {\"n_neighbors\": np.arange(1, 29, 2)}\n",
        "\n",
        "# empty list that will hold cv scores\n",
        "cv_scores = []\n",
        "\n",
        "# perform 10-fold cross validation on the KNN classifier with raw features\n",
        "print(\" 10-fold cross validation on the KNN classifier with raw features\")\n",
        "for k in params[\"n_neighbors\"]:\n",
        "    neigh = KNeighborsClassifier(n_neighbors=k)\n",
        "    cv_scores.append(np.mean(cross_val_score(neigh,Raw_train,y_train,cv=10)))\n",
        "\n",
        "print(\"Plotting the misclassification error versus k\")\n",
        "# changing to misclassification error\n",
        "MSE = [1 - x for x in cv_scores]\n",
        "\n",
        "\n",
        "# determining best k\n",
        "optimal_k =0\n",
        "for k,result in enumerate(cv_scores):\n",
        "    if result > cv_scores[optimal_k]:\n",
        "        optimal_k = k\n",
        "        \n",
        "print(\"The optimal number of neighbors is %d\" % optimal_k)\n",
        "\n",
        "# plot misclassification error vs k\n",
        "plt.plot(params[\"n_neighbors\"], MSE)\n",
        "plt.xlabel('Number of Neighbors K')\n",
        "plt.ylabel('Misclassification Error')\n",
        "plt.show()\n",
        "\n",
        "\n",
        "\n",
        "# empty list that will hold cv scores\n",
        "cv_scores = []\n",
        "\n",
        "# perform 10-fold cross validation on the KNN classifier with raw features\n",
        "print(\" 10-fold cross validation on the KNN classifier with hist features\")\n",
        "\n",
        "for k in params[\"n_neighbors\"]:\n",
        "    neigh = KNeighborsClassifier(n_neighbors=k)\n",
        "    cv_scores.append(np.mean(cross_val_score(neigh,Hist_train,y_train,cv=10)))\n",
        "    \n",
        "print(\"Plotting the misclassification error versus k\")\n",
        "    \n",
        "# changing to misclassification error\n",
        "MSE = [1 - x for x in cv_scores]\n",
        "\n",
        "# determining best k\n",
        "optimal_k =0\n",
        "for k,result in enumerate(cv_scores):\n",
        "    if result > cv_scores[optimal_k]:\n",
        "        optimal_k = k\n",
        "\n",
        "# plot misclassification error vs k\n",
        "plt.plot(params[\"n_neighbors\"], MSE)\n",
        "plt.xlabel('Number of Neighbors K')\n",
        "plt.ylabel('Misclassification Error')\n",
        "plt.show()\n",
        "\n",
        "print(\"The optimal number of neighbors is %d\" % optimal_k)\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " 10-fold cross validation on the KNN classifier with raw features\n",
            "Plotting the misclassification error versus k\n",
            "The optimal number of neighbors is 5\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAfUAAAFYCAYAAABKymUhAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nDHUiIiKJYKgTERFJxP8BLKkyrf1FlC0AAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 576x396 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            " 10-fold cross validation on the KNN classifier with hist features\n",
            "Plotting the misclassification error versus k\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAfUAAAFYCAYAAABKymUhAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 576x396 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "The optimal number of neighbors is 4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "oJ1zrUtAf2jG",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### Summary of Part 1\n",
        "\n",
        "We note that final score is not really better than hazard, and I then decided not to spend to much time on this method, since it's based on a very naive representation and should be impossible to improve\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "collapsed": true,
        "id": "oGRAcoxzf2jG",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Part 2 : using Hand-crafted features\n",
        "\n",
        "In this part, I've also followed the traditional image recognition approach but I tried to improve my image representation by using better features than the raw representation and the color distribution. Indeed, raw pixel data is hard to use for machine learning and for comparing images in general due to the different challenges explained before.\n",
        "As a consequence, the computer vision community had studied and proposed a wide range of robust and discriminative features such as HOG (Histogram of Oriented Gradients), SIFT (Scale-Invariant Feature Transform) or SURF(Speeded Up Robust Features) among others. These features are often refered as **hand-crafted features**.\n",
        "\n",
        "\n",
        "I've used the discriminative bag of visual words (BoVW) approach to represent the content of the target images.  Bag of visual words (BoVW) is a popular technique for image classification inspired by models used in natural language processing and texture recognition. BoVW downplays word arrangement (spatial information in the image) and classifies based on an histogram of the frequency of visual words in image content. The set of visual words forms a visual vocabulary, which is constructed by clustering a large corpus of features. A first step will thus consist in building a visual vocabulary by clustering a large set of local features (I've chosen to use DENSE SIFT) extracted from our training image corpus.\n",
        "\n",
        "I found this link very interesting to explain the BoVW approach : [mathwoks](https://fr.mathworks.com/help/vision/ug/image-classification-with-bag-of-visual-words.html?requestedDomain=www.mathworks.com)\n",
        "\n",
        "Then, the typical BoVW pipeline for representing an image consists in :\n",
        "1. extracting the local features from the image,\n",
        "2. encoding the local features to the corresponding visual words\n",
        "3. performing spatial pooling.\n",
        "\n",
        "Then, all the images of my training set are described with the BoVW representation and I then trained a SVM to categorize image from the representation. I firstly described only 500 image with 100 words.\n",
        "\n",
        "### Visual Vocabulary Building"
      ]
    },
    {
      "metadata": {
        "id": "ltKB7G2Tf2jH",
        "colab_type": "code",
        "outputId": "8b1a8b2c-9cc4-4448-c7a9-e40e34fc2245",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import imutils \n",
        "import numpy as np\n",
        "import os\n",
        "from imutils.paths import *\n",
        "from skimage import data,transform,io,color\n",
        "from sklearn.cluster import KMeans\n",
        "from random import shuffle\n",
        "\n",
        "# Here I extracted SIFT features on all images of the training set and made a k-means clustering to build the visual vocabulary.\n",
        "\n",
        "# Get all the path to the images and save them in a list\n",
        "# image_paths and the corresponding label in image_classes\n",
        "image_paths = []\n",
        "image_classes = []\n",
        "print('Importing images')\n",
        "imagePaths = list(list_images('./sampleDeep/train'))\n",
        "shuffle(imagePaths)\n",
        "imagePaths = imagePaths[:500]\n",
        "        \n",
        "print('SIFT')\n",
        "# Create feature extraction and keypoint detector objects using opencv\n",
        "sift = cv2.ORB_create()\n",
        "\n",
        "des_list = []\n",
        "\n",
        "for indice,i in enumerate(imagePaths):\n",
        "    image = cv2.imread(i)\n",
        "    des_list.extend(sift.detectAndCompute(image,None)[1])\n",
        "    \n",
        "print(\"K-means\")\n",
        "# Perform k-means clustering\n",
        "k = 100\n",
        "# TO COMPLETE \n",
        "kmeans = KMeans(n_clusters=k).fit(des_list)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Importing images\n",
            "SIFT\n",
            "K-means\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "fSdA8KMYuG27",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "I then had to create the vector of labels corresponding to the images selected randomly."
      ]
    },
    {
      "metadata": {
        "id": "XQ2wwjS9f2jL",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "image_classes = []\n",
        "for image in imagePaths:\n",
        "    if 'cats' in image:\n",
        "        image_classes.append(1)\n",
        "    else:\n",
        "        image_classes.append(0)\n",
        "        "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "JiVWajQgf2jP",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### BoVW pipeline \n",
        "Here I've written the BoVW pipeline and applied it to each image."
      ]
    },
    {
      "metadata": {
        "id": "9YbLPs23f2jQ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def build_histogram(descriptor,kmeans):\n",
        "    labels = kmeans.predict(descriptor)\n",
        "    return np.histogram(labels,bins=k)[0]\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "fDBJA84vf2jT",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "preprocessed_images = []\n",
        "label = []\n",
        "for i in imagePaths:\n",
        "    image = cv2.imread(i)\n",
        "    descriptor = sift.detectAndCompute(image,None)[1]\n",
        "    if descriptor is not None:\n",
        "        histogram = build_histogram(descriptor,kmeans)\n",
        "        preprocessed_images.append(list(histogram))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "qo6a1pfef2jV",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Classification using the BoVW representation and linear SVMs\n",
        "\n",
        "Now, I trained a linear SVM using BoVW representation"
      ]
    },
    {
      "metadata": {
        "id": "N752jEszf2jb",
        "colab_type": "code",
        "outputId": "d9f0dc4a-8906-471d-f182-e5a2d8d1d4e9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        }
      },
      "cell_type": "code",
      "source": [
        "from sklearn import svm\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "# I've decided to tune the params kernel, C and gamma which are very significant for a SVM\n",
        "params = {'kernel':['linear','poly','rbf','sigmoid'],'C':[0.1,1,10,100],'gamma':['auto','scale']}\n",
        "\n",
        "X_train,X_test,y_train,y_test = train_test_split(np.array(preprocessed_images),image_classes,random_state=42)\n",
        "svc = svm.SVC()\n",
        "clf = GridSearchCV(svc,params)\n",
        "clf = clf.fit(X_train,y_train)\n",
        "print(clf.best_params_)\n",
        "print(clf.score(X_test,y_test))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_split.py:2053: FutureWarning: You should specify a value for 'cv' instead of relying on the default value. The default value will change from 3 to 5 in version 0.22.\n",
            "  warnings.warn(CV_WARNING, FutureWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "{'C': 0.1, 'gamma': 'scale', 'kernel': 'poly'}\n",
            "0.64\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_search.py:841: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
            "  DeprecationWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "2bgff6Dq19d_",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "According to the Grid search CV, the best parameters found are : C=0.1, gamma = 'scale' and a polynomial kernel. We will then use these parameters and it gave us a score of **0.64**, which is better than the previous score. \n",
        "This score can surely be improved, tuning more the model, testing other models, training our models on the entire dataset and using more visual words. \n",
        "But since these tasks are quite time demanding, I prefered focus on Deep Learning and CNN."
      ]
    },
    {
      "metadata": {
        "id": "23kodqcnf2jf",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Part 3 : using Convolutional Neural Networks\n",
        "\n",
        "In this part, I've used convolutional neural networks (CNNs) and deep learning in order to build your image classifier. I've used the [Keras framework](https://keras.io/) which is a high-level neural networks API, written in Python and capable of running on top of [TensorFlow](https://www.tensorflow.org/). It was developed with a focus on enabling fast experimentation and as a consequence it is a good choice for this course.\n",
        "\n",
        "\n",
        "### Keras with sample data from the Dogs and cats recognition challenge\n",
        "\n",
        "In this part, I have used Keras in order to\n",
        " + Build and train a small network from scratch\n",
        " + Use the bottleneck features of a pre-trained network\n",
        " + Fine-tune the top layers of a pre-trained network\n",
        "\n",
        "I have done this work on a sample Dataset (folder sample Deep) of the initial Kaggle challenge that contains a training set composed of 1000 images of cats and 1000 images of dogs and a validation set, used to evaluate our models that contains 400 additional samples from each class.\n",
        "\n",
        "At the end, I could apply this approach on the whole dataset but this require other computing resources than my computer.\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "p7D0VvMtf2jg",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### Data preparation and loading\n",
        "\n",
        "As for the previous classifiers, data preparation is also required when working with convolutional neural networks and deep learning models. You will use the [*ImageDataGenerator class*](https://keras.io/preprocessing/image/) that defines the configuration for image data preparation. This class allows us to make data augmentation as we will see later, which is often necessary in Deep Learning\n",
        " This is the case of the sampleDeep dataset.  I chose the batch size testing multiple ones and choosing the one with the best efficiency.\n",
        " "
      ]
    },
    {
      "metadata": {
        "id": "bV9hDAQYf2jh",
        "colab_type": "code",
        "outputId": "f6157f80-c4d5-4e8a-dadb-69f0071b7d09",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "cell_type": "code",
      "source": [
        "# without augmentation, only rescaling\n",
        "\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "# definition of the number of samples propagated through the network at each step\n",
        "batch_size = 8\n",
        "\n",
        "# dimensions of our images.\n",
        "img_width, img_height = 150, 150\n",
        "\n",
        "train_data_dir = 'sampleDeep/train'\n",
        "validation_data_dir = 'sampleDeep/valid'\n",
        "\n",
        "# create and configure an ImageDataGenerator for the training data with only rescaling to 0..1\n",
        "train_datagen =  ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "        train_data_dir,  # the target directory\n",
        "        target_size=(img_width, img_height),  # all images will be resized to 150x150\n",
        "        batch_size=batch_size,\n",
        "        class_mode='binary')\n",
        "\n",
        "# create and configure an ImageDataGenerator for the validation data with only rescaling to 0..1\n",
        "test_datagen =  ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "valid_generator = test_datagen.flow_from_directory(\n",
        "        validation_data_dir,  # the target directory\n",
        "        target_size=(img_width, img_height),  # all images will be resized to 150x150\n",
        "        batch_size=batch_size,\n",
        "        class_mode='binary') \n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 2010 images belonging to 2 classes.\n",
            "Found 800 images belonging to 2 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "q2MsKdHpf2jm",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Build a model from scratch\n",
        "\n",
        "### Training our first model\n",
        "\n",
        "Here, I have built a convolutional neural network which is ,by design, one of the best models available for most \"perceptual\" problems (such as image classification), even with very little data to learn from.\n",
        "My first network model consists in 2 convolutional layers followed by a fully-connected layer. As loss and metrics, I used binary crossentropy and accuracy which are the most adapted to binary classification. As optimizer, we tried multiple ones such as adam, sgd and rmsprop and rmsprop was found as the more efficient for this project.\n",
        "\n",
        "This model was really slow to train, I then added Max Pooling layers to fasten the training.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "LZ9G97Bef2jn",
        "colab_type": "code",
        "outputId": "70885ba2-f804-4afe-a9b4-a2c8bf13ff6e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 156
        }
      },
      "cell_type": "code",
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Conv2D, MaxPooling2D\n",
        "from keras.layers import Activation, Dropout, Flatten, Dense\n",
        "\n",
        "model = Sequential()\n",
        "\n",
        "# First convolutional layer\n",
        "\n",
        "model.add(Conv2D(filters=64,input_shape=(150,150,3),kernel_size=(3,3)))\n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPooling2D(pool_size = (2,2)))\n",
        "\n",
        "# Second  convolutional layer\n",
        "model.add(Conv2D(filters=64,kernel_size=(3,3)))\n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPooling2D(pool_size = (2,2)))\n",
        "\n",
        "# Adding of one fully-connected layer \n",
        "model.add(Flatten())\n",
        "model.add(Dense(128))\n",
        "model.add(Activation('relu'))\n",
        "\n",
        "# single unit and sigmoid activation, which is perfect for a binary classification. \n",
        "\n",
        "model.add(Dense(1))\n",
        "model.add(Activation('sigmoid'))\n",
        "\n",
        "# Use of the binary_crossentropy loss to train our model, of the rmsprop optimizer and the accuracy metrics\n",
        "model.compile(optimizer='rmsprop',\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "model.fit_generator(train_generator,steps_per_epoch=2000//batch_size,epochs=3,validation_data=valid_generator,validation_steps=800//batch_size)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/3\n",
            "250/250 [==============================] - 197s 787ms/step - loss: 0.7273 - acc: 0.5870 - val_loss: 0.6375 - val_acc: 0.6488\n",
            "Epoch 2/3\n",
            "250/250 [==============================] - 192s 770ms/step - loss: 0.6242 - acc: 0.6850 - val_loss: 0.6487 - val_acc: 0.6613\n",
            "Epoch 3/3\n",
            "250/250 [==============================] - 191s 765ms/step - loss: 0.4920 - acc: 0.7810 - val_loss: 0.9406 - val_acc: 0.6525\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f59333f6550>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "metadata": {
        "id": "I4AkPCl5MRCi",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The previous model gave us an accuracy of 0.78 over the train set and 0.65 over the validation set. I added new layers to enable the model to capt new informations. To train this deeper model, I raised the number of epochs to 5.\n"
      ]
    },
    {
      "metadata": {
        "id": "vs_PbO7iM7-5",
        "colab_type": "code",
        "outputId": "9d860bf6-e281-4766-fda9-54f42efbb40c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 224
        }
      },
      "cell_type": "code",
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Conv2D, MaxPooling2D\n",
        "from keras.layers import Activation, Dropout, Flatten, Dense\n",
        "\n",
        "model = Sequential()\n",
        "\n",
        "# First convolutional layer\n",
        "\n",
        "model.add(Conv2D(filters=64,input_shape=(150,150,3),kernel_size=(3,3)))\n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPooling2D(pool_size = (2,2)))\n",
        "\n",
        "# Second  convolutional layer\n",
        "model.add(Conv2D(filters=64,kernel_size=(3,3)))\n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPooling2D(pool_size = (2,2)))\n",
        "\n",
        "model.add(Conv2D(filters=64,kernel_size=(3,3)))\n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPooling2D(pool_size = (2,2)))\n",
        "\n",
        "# Adding of one fully-connected layer \n",
        "model.add(Flatten())\n",
        "model.add(Dense(128))\n",
        "model.add(Activation('relu'))\n",
        "\n",
        "model.add(Dense(64))\n",
        "model.add(Activation('relu'))\n",
        "\n",
        "# single unit and sigmoid activation, which is perfect for a binary classification. \n",
        "\n",
        "model.add(Dense(1))\n",
        "model.add(Activation('sigmoid'))\n",
        "\n",
        "# Use of the binary_crossentropy loss to train our model, of the rmsprop optimizer and the accuracy metrics\n",
        "model.compile(optimizer='rmsprop',\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "model.fit_generator(train_generator,steps_per_epoch=2000//batch_size,epochs=5,validation_data=valid_generator,validation_steps=800//batch_size)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "250/250 [==============================] - 189s 757ms/step - loss: 0.7203 - acc: 0.5170 - val_loss: 0.8578 - val_acc: 0.5012\n",
            "Epoch 2/5\n",
            "250/250 [==============================] - 187s 747ms/step - loss: 0.6581 - acc: 0.6255 - val_loss: 0.6366 - val_acc: 0.6250\n",
            "Epoch 3/5\n",
            "250/250 [==============================] - 187s 746ms/step - loss: 0.6159 - acc: 0.6680 - val_loss: 0.5798 - val_acc: 0.6787\n",
            "Epoch 4/5\n",
            "250/250 [==============================] - 183s 733ms/step - loss: 0.5621 - acc: 0.7220 - val_loss: 0.6295 - val_acc: 0.6963\n",
            "Epoch 5/5\n",
            "250/250 [==============================] - 184s 737ms/step - loss: 0.4898 - acc: 0.7655 - val_loss: 0.6583 - val_acc: 0.6863\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f5935f990b8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "metadata": {
        "id": "OtlVq_i5d1u6",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "As expected, the performance improved to 68,63% on the validation set, but the score on the validation set is still pretty far for the score on the train test (76,55%). I added Dropout layers to prevent the model from overfitting."
      ]
    },
    {
      "metadata": {
        "id": "OwK8NmjGdlwA",
        "colab_type": "code",
        "outputId": "689af9e0-7441-4470-b85c-493e2f7282e4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 224
        }
      },
      "cell_type": "code",
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Conv2D, MaxPooling2D\n",
        "from keras.layers import Activation, Dropout, Flatten, Dense\n",
        "\n",
        "model = Sequential()\n",
        "\n",
        "# First convolutional layer\n",
        "\n",
        "model.add(Conv2D(filters=64,input_shape=(150,150,3),kernel_size=(3,3)))\n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPooling2D(pool_size = (2,2)))\n",
        "\n",
        "# Second  convolutional layer\n",
        "model.add(Conv2D(filters=64,kernel_size=(3,3)))\n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPooling2D(pool_size = (2,2)))\n",
        "\n",
        "model.add(Conv2D(filters=64,kernel_size=(3,3)))\n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPooling2D(pool_size = (2,2)))\n",
        "\n",
        "# Adding of one fully-connected layer \n",
        "model.add(Flatten())\n",
        "model.add(Dense(128))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dropout(rate=0.2))\n",
        "\n",
        "model.add(Dense(64))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dropout(rate=0.2))\n",
        "\n",
        "# single unit and sigmoid activation, which is perfect for a binary classification. \n",
        "\n",
        "model.add(Dense(1))\n",
        "model.add(Activation('sigmoid'))\n",
        "\n",
        "# Use of the binary_crossentropy loss to train our model, of the rmsprop optimizer and the accuracy metrics\n",
        "model.compile(optimizer='rmsprop',\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "model.fit_generator(train_generator,steps_per_epoch=2000//batch_size,epochs=5,validation_data=valid_generator,validation_steps=800//batch_size)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "250/250 [==============================] - 179s 716ms/step - loss: 0.7231 - acc: 0.5070 - val_loss: 0.6850 - val_acc: 0.4838\n",
            "Epoch 2/5\n",
            "250/250 [==============================] - 178s 712ms/step - loss: 0.6833 - acc: 0.5870 - val_loss: 0.6203 - val_acc: 0.6637\n",
            "Epoch 3/5\n",
            "250/250 [==============================] - 178s 713ms/step - loss: 0.6388 - acc: 0.6500 - val_loss: 0.6012 - val_acc: 0.6725\n",
            "Epoch 4/5\n",
            "250/250 [==============================] - 179s 718ms/step - loss: 0.6002 - acc: 0.6925 - val_loss: 0.8028 - val_acc: 0.6138\n",
            "Epoch 5/5\n",
            "250/250 [==============================] - 177s 707ms/step - loss: 0.5716 - acc: 0.7045 - val_loss: 0.5799 - val_acc: 0.7150\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f59343b10f0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "metadata": {
        "id": "j29-tq5elopB",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "We notice that the model is not overfitting anymore, but we have a larger bias on the training set. However the score is better and reach 70,45% on the training set and 71,5% on the validation set."
      ]
    },
    {
      "metadata": {
        "id": "YRQeOGJ3f2jj",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Only few training examples are available in the sampleDeep dataset. In order to make the most of these training examples, a current approach is to **augment** them via a number of random transformations, so that our model would never see twice the exact same picture. This augmentation step also helps prevent overfitting and helps the model generalize better.\n"
      ]
    },
    {
      "metadata": {
        "id": "Vy3CyT-Pf2jp",
        "colab_type": "code",
        "outputId": "670f36d1-321a-4601-c52f-0cc6bb07776b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "cell_type": "code",
      "source": [
        "# augmentation configuration use for training:\n",
        "train_datagen = ImageDataGenerator(\n",
        "    rescale=1. / 255,\n",
        "    shear_range=0.2,\n",
        "    zoom_range=0.2,\n",
        "    horizontal_flip=True)\n",
        "\n",
        "# augmentation configuration use for testing:\n",
        "# only rescaling\n",
        "test_datagen = ImageDataGenerator(rescale=1. / 255)\n",
        "\n",
        "# fit the generator to your data\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "    train_data_dir,\n",
        "    target_size=(img_width, img_height),\n",
        "    batch_size=batch_size,\n",
        "    class_mode='binary')\n",
        "\n",
        "valid_generator = test_datagen.flow_from_directory(\n",
        "    validation_data_dir,\n",
        "    target_size=(img_width, img_height),\n",
        "    batch_size=batch_size,\n",
        "    class_mode='binary')\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 2010 images belonging to 2 classes.\n",
            "Found 800 images belonging to 2 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "N6XTx1CK4s35",
        "colab_type": "code",
        "outputId": "bee20cd2-8d4f-49b4-c6d6-10715e367df6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 292
        }
      },
      "cell_type": "code",
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Conv2D, MaxPooling2D\n",
        "from keras.layers import Activation, Dropout, Flatten, Dense\n",
        "from keras.optimizers import RMSprop\n",
        "\n",
        "model = Sequential()\n",
        "\n",
        "# First convolutional layer\n",
        "\n",
        "model.add(Conv2D(filters=64,input_shape=(150,150,3),kernel_size=(3,3)))\n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPooling2D(pool_size = (2,2)))\n",
        "\n",
        "# Second  convolutional layer\n",
        "model.add(Conv2D(filters=64,kernel_size=(3,3)))\n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPooling2D(pool_size = (2,2)))\n",
        "\n",
        "model.add(Conv2D(filters=64,kernel_size=(3,3)))\n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPooling2D(pool_size = (2,2)))\n",
        "\n",
        "# Adding of one fully-connected layer \n",
        "model.add(Flatten())\n",
        "model.add(Dense(128))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dropout(rate=0.2))\n",
        "\n",
        "model.add(Dense(64))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dropout(rate=0.2))\n",
        "\n",
        "# single unit and sigmoid activation, which is perfect for a binary classification. \n",
        "\n",
        "model.add(Dense(1))\n",
        "model.add(Activation('sigmoid'))\n",
        "# Use of the binary_crossentropy loss to train our model, of the rmsprop optimizer and the accuracy metrics\n",
        "model.compile(optimizer='rmsprop',\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "model.fit_generator(train_generator,steps_per_epoch=2000//batch_size,epochs=7,validation_data=valid_generator,validation_steps=800//batch_size)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/7\n",
            "250/250 [==============================] - 182s 729ms/step - loss: 1.2372 - acc: 0.5010 - val_loss: 0.6875 - val_acc: 0.5925\n",
            "Epoch 2/7\n",
            "250/250 [==============================] - 180s 719ms/step - loss: 0.6892 - acc: 0.5545 - val_loss: 0.6319 - val_acc: 0.6100\n",
            "Epoch 3/7\n",
            "250/250 [==============================] - 180s 720ms/step - loss: 0.6586 - acc: 0.6405 - val_loss: 0.6141 - val_acc: 0.6813\n",
            "Epoch 4/7\n",
            "250/250 [==============================] - 181s 724ms/step - loss: 0.6288 - acc: 0.6860 - val_loss: 0.5717 - val_acc: 0.7225\n",
            "Epoch 5/7\n",
            "250/250 [==============================] - 181s 724ms/step - loss: 0.5630 - acc: 0.7120 - val_loss: 0.5749 - val_acc: 0.6913\n",
            "Epoch 6/7\n",
            "250/250 [==============================] - 180s 721ms/step - loss: 0.5493 - acc: 0.7420 - val_loss: 0.6372 - val_acc: 0.7050\n",
            "Epoch 7/7\n",
            "250/250 [==============================] - 181s 725ms/step - loss: 0.4921 - acc: 0.7735 - val_loss: 0.6552 - val_acc: 0.6613\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f592c7a9e48>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "metadata": {
        "id": "43A1fDv-f2js",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Once again, our model overfits. To improve our score, we would need to train the model over the whole dataset and we could expect a great improvement since the dataset is much bigger than the sample we used. However, my computing resources aren't sufficient to do it so we'll stay with this score for now."
      ]
    },
    {
      "metadata": {
        "id": "evT6E2PRf2jx",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Use a pretrained Convnet model\n",
        "\n",
        "In fact, what is really slow is training the convolutionnal layers since it has to understand features contained in the image. Moreover, we don't have enough data to train it . Instead, in image classification, it is common to use networks pre-trained on a large dataset (such as ImageNet)  and to use it either as an initialization of as a fixed feature extractor for the task of interest (**transfer learning**). Indeed, these networks have already learned features that are useful for most computer vision problems, and leveraging such features would allow us to reach a better accuracy than any method that would only rely on the available data.\n",
        "\n",
        "Different strategies can be used in transfer learning scenarios :\n",
        "\n",
        "1. The ConvNet, trained on a large image dataset such as Imagenet, is used as a fixed feature extractor. In this case, the pipeline consists in taking the pre-trained ConvNet, removing the last fully connected layer and that by treating the rest of the ConvNet architecture as a fixed feature extractor for the new dataset\n",
        "2. Fine Tuning of the ConvNet. In this case,  the weights of a part of the pretrained network are fine-tuned by continuing the backpropagation. As it as been observed that the first features of a ConvNet contain more generic features (e.g. edge detectors or color blob detectors) that should be useful to many tasks and that later layers become progressively more specific to the details of the classes contained in the original dataset, only a higher portion of the network is fine-tuned.\n",
        "\n",
        "I tried both methods bu the second one was very expensive in resources and didn't show a real improvement compared to the first one, so I decided to keep only the first method in this Notebook\n",
        "\n",
        "### ConvNet as a fixed feature extractor   \n",
        "\n",
        "In particular, we will use the VGG16 architecture which won the 2014 Imagenet competition, and is a very simple model to create and understand. The VGG Imagenet team created both a larger, slower, slightly more accurate model (VGG 19) and a smaller, faster model (VGG 16). We will be using VGG 16 since the much slower performance of VGG19 is generally not worth the very minor improvement in accuracy.\n",
        "\n",
        "In the code below, the strategy will consist in instantiating only the convolutional part of the model (using the *include_top* argument) (see the and in running this model on our own training and validation data once by recording the output in two numpy arrays. Then, I have trained a small fully-connected model on top of the stored features.\n",
        " \n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "FaVDMBVmf2jx",
        "colab_type": "code",
        "outputId": "869edf3e-fc9a-4bca-ad7d-95e28d3b3fe9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1873
        }
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dropout, Flatten, Dense, Activation\n",
        "from keras import applications\n",
        "\n",
        "\n",
        "# dimensions of our images.\n",
        "img_width, img_height = 150, 150\n",
        "\n",
        "top_model_weights_path = './weigths_models/bottleneck_fc_model.h5'\n",
        "train_data_dir = 'sampleDeep/train'\n",
        "validation_data_dir = 'sampleDeep/valid'\n",
        "nb_train_samples = 2010\n",
        "nb_validation_samples = 800\n",
        "epochs = 50\n",
        "batch_size = 8\n",
        "\n",
        "\n",
        "# Function that instanciates the convolutional part of the VGG16 pre-trained model on Imagenet and that runs it on our training and validation data\n",
        "\n",
        "def save_bottlebeck_features():\n",
        "    datagen = ImageDataGenerator(rescale=1. / 255)\n",
        "\n",
        "    # build and load the VGG16 network without the fully connected layers\n",
        "    vgg = applications.vgg16.VGG16(include_top=False,input_shape=(img_width,img_height,3),classes=2)\n",
        "    # preparation of the training data\n",
        "    generator = datagen.flow_from_directory(\n",
        "        train_data_dir,\n",
        "        target_size=(img_width, img_height),\n",
        "        batch_size=batch_size,\n",
        "        class_mode=None,\n",
        "        shuffle=False)\n",
        "    \n",
        "    print('Prediction 1')\n",
        "    # Generation of the predictions for the input samples from the training data generator and return them as a numpy array that we can saved\n",
        "    bottleneck_features_train = vgg.predict_generator(generator,steps=len(generator))\n",
        "    np.save(open('./weigths_models/bottleneck_features_train.npy', 'wb'),\n",
        "            bottleneck_features_train)\n",
        "\n",
        "    # preparation of the validation data\n",
        "    valid_generator = datagen.flow_from_directory(\n",
        "    validation_data_dir,\n",
        "    target_size=(img_width, img_height),\n",
        "    batch_size=batch_size,\n",
        "    class_mode=None,\n",
        "    shuffle=False\n",
        "    )\n",
        "    print('Prediction 2')\n",
        "    # Generation of the predictions for the input samples from the validation data generator and return them as a numpy array that we can saved\n",
        "    bottleneck_features_validation = vgg.predict_generator(valid_generator,steps=len(valid_generator))\n",
        "    np.save(open('./weigths_models/bottleneck_features_validation.npy', 'wb'),\n",
        "            bottleneck_features_validation)\n",
        "\n",
        "    \n",
        "# Function that trains a small fully-connected model on top of the stored previous features\n",
        "    \n",
        "    \n",
        "def train_top_model():\n",
        "    train_data = np.load(open('./weigths_models/bottleneck_features_train.npy','rb'))\n",
        "    train_labels = np.array(\n",
        "        [0] * (nb_train_samples // 2) + [1] * (nb_train_samples // 2))\n",
        "\n",
        "    validation_data = np.load(open('./weigths_models/bottleneck_features_validation.npy','rb'))\n",
        "    validation_labels = np.array(\n",
        "        [0] * (nb_validation_samples // 2) + [1] * (nb_validation_samples // 2))\n",
        "\n",
        "    # Building of the small fully-connected model\n",
        "    model = Sequential()\n",
        "    \n",
        "    model.add(Flatten())\n",
        "    model.add(Dense(32))\n",
        "    model.add(Activation('relu'))\n",
        "    \n",
        "    model.add(Dense(1,activation='sigmoid'))\n",
        "\n",
        "    # Configuration of the learning process\n",
        "    model.compile(optimizer='rmsprop',\n",
        "                  loss='binary_crossentropy', metrics=['accuracy'])\n",
        "    print('Training')\n",
        "    # Training of the model\n",
        "    model.fit(x=train_data,y=train_labels,validation_data=(validation_data,validation_labels),epochs = epochs)\n",
        "    model.save_weights('./weigths_models/top_layers.h5')\n",
        "\n",
        "save_bottlebeck_features()\n",
        "train_top_model()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://github.com/fchollet/deep-learning-models/releases/download/v0.1/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "58892288/58889256 [==============================] - 1s 0us/step\n",
            "Found 2010 images belonging to 2 classes.\n",
            "Prediction 1\n",
            "Found 800 images belonging to 2 classes.\n",
            "Prediction 2\n",
            "Training\n",
            "Train on 2010 samples, validate on 800 samples\n",
            "Epoch 1/50\n",
            "2010/2010 [==============================] - 4s 2ms/step - loss: 0.5723 - acc: 0.7726 - val_loss: 0.2892 - val_acc: 0.8738\n",
            "Epoch 2/50\n",
            "2010/2010 [==============================] - 1s 263us/step - loss: 0.3155 - acc: 0.8687 - val_loss: 0.3202 - val_acc: 0.8488\n",
            "Epoch 3/50\n",
            "2010/2010 [==============================] - 1s 267us/step - loss: 0.2239 - acc: 0.9090 - val_loss: 0.2707 - val_acc: 0.8750\n",
            "Epoch 4/50\n",
            "2010/2010 [==============================] - 1s 260us/step - loss: 0.1965 - acc: 0.9174 - val_loss: 0.2531 - val_acc: 0.9012\n",
            "Epoch 5/50\n",
            "2010/2010 [==============================] - 1s 264us/step - loss: 0.1443 - acc: 0.9403 - val_loss: 0.2902 - val_acc: 0.8775\n",
            "Epoch 6/50\n",
            "2010/2010 [==============================] - 1s 263us/step - loss: 0.1352 - acc: 0.9428 - val_loss: 0.2734 - val_acc: 0.8850\n",
            "Epoch 7/50\n",
            "2010/2010 [==============================] - 1s 259us/step - loss: 0.1031 - acc: 0.9632 - val_loss: 0.3987 - val_acc: 0.8438\n",
            "Epoch 8/50\n",
            "2010/2010 [==============================] - 1s 288us/step - loss: 0.0854 - acc: 0.9692 - val_loss: 0.3227 - val_acc: 0.8675\n",
            "Epoch 9/50\n",
            "2010/2010 [==============================] - 1s 291us/step - loss: 0.0807 - acc: 0.9657 - val_loss: 0.2931 - val_acc: 0.8862\n",
            "Epoch 10/50\n",
            "2010/2010 [==============================] - 1s 268us/step - loss: 0.0495 - acc: 0.9786 - val_loss: 0.3655 - val_acc: 0.8625\n",
            "Epoch 11/50\n",
            "2010/2010 [==============================] - 1s 268us/step - loss: 0.0474 - acc: 0.9841 - val_loss: 0.3112 - val_acc: 0.8862\n",
            "Epoch 12/50\n",
            "2010/2010 [==============================] - 1s 267us/step - loss: 0.0358 - acc: 0.9886 - val_loss: 0.3219 - val_acc: 0.8912\n",
            "Epoch 13/50\n",
            "2010/2010 [==============================] - 1s 266us/step - loss: 0.0319 - acc: 0.9920 - val_loss: 0.3543 - val_acc: 0.8850\n",
            "Epoch 14/50\n",
            "2010/2010 [==============================] - 1s 255us/step - loss: 0.0194 - acc: 0.9940 - val_loss: 0.5407 - val_acc: 0.8362\n",
            "Epoch 15/50\n",
            "2010/2010 [==============================] - 1s 258us/step - loss: 0.0196 - acc: 0.9950 - val_loss: 0.3816 - val_acc: 0.8862\n",
            "Epoch 16/50\n",
            "2010/2010 [==============================] - 1s 263us/step - loss: 0.0157 - acc: 0.9970 - val_loss: 0.4061 - val_acc: 0.8812\n",
            "Epoch 17/50\n",
            "2010/2010 [==============================] - 1s 262us/step - loss: 0.0174 - acc: 0.9940 - val_loss: 0.4148 - val_acc: 0.8900\n",
            "Epoch 18/50\n",
            "2010/2010 [==============================] - 1s 265us/step - loss: 0.0078 - acc: 0.9970 - val_loss: 0.3984 - val_acc: 0.8912\n",
            "Epoch 19/50\n",
            "2010/2010 [==============================] - 1s 271us/step - loss: 0.0131 - acc: 0.9955 - val_loss: 0.4153 - val_acc: 0.8837\n",
            "Epoch 20/50\n",
            "2010/2010 [==============================] - 1s 269us/step - loss: 0.0134 - acc: 0.9960 - val_loss: 0.4234 - val_acc: 0.8825\n",
            "Epoch 21/50\n",
            "2010/2010 [==============================] - 1s 264us/step - loss: 0.0165 - acc: 0.9945 - val_loss: 0.4616 - val_acc: 0.8788\n",
            "Epoch 22/50\n",
            "2010/2010 [==============================] - 1s 270us/step - loss: 0.0181 - acc: 0.9935 - val_loss: 0.5204 - val_acc: 0.8663\n",
            "Epoch 23/50\n",
            "2010/2010 [==============================] - 1s 258us/step - loss: 8.3201e-04 - acc: 1.0000 - val_loss: 0.5118 - val_acc: 0.8738\n",
            "Epoch 24/50\n",
            "2010/2010 [==============================] - 1s 262us/step - loss: 0.0200 - acc: 0.9935 - val_loss: 0.4727 - val_acc: 0.8825\n",
            "Epoch 25/50\n",
            "2010/2010 [==============================] - 1s 266us/step - loss: 0.0141 - acc: 0.9935 - val_loss: 0.4568 - val_acc: 0.8775\n",
            "Epoch 26/50\n",
            "2010/2010 [==============================] - 1s 264us/step - loss: 4.1710e-04 - acc: 1.0000 - val_loss: 1.1562 - val_acc: 0.8175\n",
            "Epoch 27/50\n",
            "2010/2010 [==============================] - 1s 265us/step - loss: 0.0102 - acc: 0.9975 - val_loss: 0.5014 - val_acc: 0.8825\n",
            "Epoch 28/50\n",
            "2010/2010 [==============================] - 1s 258us/step - loss: 0.0078 - acc: 0.9970 - val_loss: 0.4987 - val_acc: 0.8825\n",
            "Epoch 29/50\n",
            "2010/2010 [==============================] - 1s 262us/step - loss: 0.0102 - acc: 0.9960 - val_loss: 0.4955 - val_acc: 0.8862\n",
            "Epoch 30/50\n",
            "2010/2010 [==============================] - 1s 265us/step - loss: 1.7338e-04 - acc: 1.0000 - val_loss: 0.5199 - val_acc: 0.8800\n",
            "Epoch 31/50\n",
            "2010/2010 [==============================] - 1s 272us/step - loss: 0.0117 - acc: 0.9965 - val_loss: 0.5602 - val_acc: 0.8775\n",
            "Epoch 32/50\n",
            "2010/2010 [==============================] - 1s 265us/step - loss: 0.0034 - acc: 0.9985 - val_loss: 0.5375 - val_acc: 0.8812\n",
            "Epoch 33/50\n",
            "2010/2010 [==============================] - 1s 265us/step - loss: 1.3186e-04 - acc: 1.0000 - val_loss: 0.5572 - val_acc: 0.8788\n",
            "Epoch 34/50\n",
            "2010/2010 [==============================] - 1s 264us/step - loss: 0.0082 - acc: 0.9965 - val_loss: 0.5410 - val_acc: 0.8800\n",
            "Epoch 35/50\n",
            "2010/2010 [==============================] - 1s 275us/step - loss: 8.3534e-05 - acc: 1.0000 - val_loss: 0.5619 - val_acc: 0.8812\n",
            "Epoch 36/50\n",
            "2010/2010 [==============================] - 1s 268us/step - loss: 0.0012 - acc: 0.9995 - val_loss: 0.5972 - val_acc: 0.8850\n",
            "Epoch 37/50\n",
            "2010/2010 [==============================] - 1s 264us/step - loss: 0.0130 - acc: 0.9955 - val_loss: 0.7158 - val_acc: 0.8663\n",
            "Epoch 38/50\n",
            "2010/2010 [==============================] - 1s 268us/step - loss: 7.4132e-05 - acc: 1.0000 - val_loss: 0.6067 - val_acc: 0.8825\n",
            "Epoch 39/50\n",
            "2010/2010 [==============================] - 1s 264us/step - loss: 0.0038 - acc: 0.9985 - val_loss: 0.5807 - val_acc: 0.8812\n",
            "Epoch 40/50\n",
            "2010/2010 [==============================] - 1s 265us/step - loss: 5.8227e-05 - acc: 1.0000 - val_loss: 0.5961 - val_acc: 0.8788\n",
            "Epoch 41/50\n",
            "2010/2010 [==============================] - 1s 264us/step - loss: 0.0066 - acc: 0.9965 - val_loss: 0.6238 - val_acc: 0.8788\n",
            "Epoch 42/50\n",
            "2010/2010 [==============================] - 1s 265us/step - loss: 2.3828e-05 - acc: 1.0000 - val_loss: 0.6216 - val_acc: 0.8700\n",
            "Epoch 43/50\n",
            "2010/2010 [==============================] - 1s 264us/step - loss: 0.0032 - acc: 0.9980 - val_loss: 0.7790 - val_acc: 0.8625\n",
            "Epoch 44/50\n",
            "2010/2010 [==============================] - 1s 264us/step - loss: 2.8121e-05 - acc: 1.0000 - val_loss: 0.6473 - val_acc: 0.8812\n",
            "Epoch 45/50\n",
            "2010/2010 [==============================] - 1s 260us/step - loss: 0.0032 - acc: 0.9990 - val_loss: 1.0344 - val_acc: 0.8462\n",
            "Epoch 46/50\n",
            "2010/2010 [==============================] - 1s 265us/step - loss: 7.5021e-05 - acc: 1.0000 - val_loss: 0.6793 - val_acc: 0.8762\n",
            "Epoch 47/50\n",
            "2010/2010 [==============================] - 1s 262us/step - loss: 9.2626e-06 - acc: 1.0000 - val_loss: 0.6616 - val_acc: 0.8812\n",
            "Epoch 48/50\n",
            "2010/2010 [==============================] - 1s 262us/step - loss: 0.0057 - acc: 0.9990 - val_loss: 0.6714 - val_acc: 0.8812\n",
            "Epoch 49/50\n",
            "2010/2010 [==============================] - 1s 266us/step - loss: 8.2258e-06 - acc: 1.0000 - val_loss: 0.6557 - val_acc: 0.8850\n",
            "Epoch 50/50\n",
            "2010/2010 [==============================] - 1s 263us/step - loss: 0.0020 - acc: 0.9995 - val_loss: 0.6513 - val_acc: 0.8850\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "2NCXma7lu_8H",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "With this simple model, we already obtain a significative improvement with a validation score of 88,5% (train score of 99,6%). It then seems that the model is largely overfitting, we need to add Dropout layers once again."
      ]
    },
    {
      "metadata": {
        "id": "9P3XFFrMIilQ",
        "colab_type": "code",
        "outputId": "638a27bb-7095-4f65-939c-31214b01860d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1771
        }
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dropout, Flatten, Dense, Activation\n",
        "from keras import applications\n",
        "\n",
        "\n",
        "# dimensions of our images.\n",
        "img_width, img_height = 150, 150\n",
        "\n",
        "top_model_weights_path = 'bottleneck_fc_model.h5'\n",
        "train_data_dir = 'sampleDeep/train'\n",
        "validation_data_dir = 'sampleDeep/valid'\n",
        "nb_train_samples = 2010\n",
        "nb_validation_samples = 800\n",
        "epochs = 50\n",
        "batch_size = 8\n",
        "    \n",
        "def train_top_model():\n",
        "    train_data = np.load(open('./weigths_models/bottleneck_features_train.npy','rb'))\n",
        "    train_labels = np.array(\n",
        "        [0] * (nb_train_samples // 2) + [1] * (nb_train_samples // 2))\n",
        "\n",
        "    validation_data = np.load(open('./weigths_models/bottleneck_features_validation.npy','rb'))\n",
        "    validation_labels = np.array(\n",
        "        [0] * (nb_validation_samples // 2) + [1] * (nb_validation_samples // 2))\n",
        "\n",
        "    # Building of the small fully-connected model\n",
        "    model = Sequential()\n",
        "    \n",
        "    model.add(Flatten())\n",
        "    model.add(Dense(32))\n",
        "    model.add(Activation('relu'))\n",
        "    model.add(Dropout(rate=0.4))\n",
        "    \n",
        "    model.add(Dense(64))\n",
        "    model.add(Activation('relu'))\n",
        "    \n",
        "    model.add(Dense(32))\n",
        "    model.add(Activation('relu'))\n",
        "    model.add(Dropout(rate=0.4))\n",
        "\n",
        "    model.add(Dense(1,activation='sigmoid'))\n",
        "\n",
        "    # Configuration of the learning process\n",
        "    model.compile(optimizer='rmsprop',\n",
        "                  loss='binary_crossentropy', metrics=['accuracy'])\n",
        "    print('Training')\n",
        "    # Training of the model\n",
        "    model.fit(x=train_data,y=train_labels,validation_data=(validation_data,validation_labels),epochs = epochs)\n",
        "    model.save_weights('./weigths_models/top_layers.h5')\n",
        "\n",
        "train_top_model()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training\n",
            "Train on 2010 samples, validate on 800 samples\n",
            "Epoch 1/50\n",
            "2010/2010 [==============================] - 8s 4ms/step - loss: 0.6138 - acc: 0.6632 - val_loss: 0.4325 - val_acc: 0.8113\n",
            "Epoch 2/50\n",
            "2010/2010 [==============================] - 1s 273us/step - loss: 0.4448 - acc: 0.8179 - val_loss: 0.3198 - val_acc: 0.8550\n",
            "Epoch 3/50\n",
            "2010/2010 [==============================] - 1s 270us/step - loss: 0.3546 - acc: 0.8607 - val_loss: 0.2705 - val_acc: 0.9000\n",
            "Epoch 4/50\n",
            "2010/2010 [==============================] - 1s 276us/step - loss: 0.3183 - acc: 0.8682 - val_loss: 0.2949 - val_acc: 0.8562\n",
            "Epoch 5/50\n",
            "2010/2010 [==============================] - 1s 270us/step - loss: 0.2753 - acc: 0.8960 - val_loss: 0.3269 - val_acc: 0.8362\n",
            "Epoch 6/50\n",
            "2010/2010 [==============================] - 1s 292us/step - loss: 0.2566 - acc: 0.9015 - val_loss: 0.2592 - val_acc: 0.9062\n",
            "Epoch 7/50\n",
            "2010/2010 [==============================] - 1s 276us/step - loss: 0.2382 - acc: 0.9144 - val_loss: 0.2732 - val_acc: 0.8738\n",
            "Epoch 8/50\n",
            "2010/2010 [==============================] - 1s 276us/step - loss: 0.2166 - acc: 0.9119 - val_loss: 0.3259 - val_acc: 0.8788\n",
            "Epoch 9/50\n",
            "2010/2010 [==============================] - 1s 283us/step - loss: 0.1969 - acc: 0.9219 - val_loss: 0.3048 - val_acc: 0.8925\n",
            "Epoch 10/50\n",
            "2010/2010 [==============================] - 1s 281us/step - loss: 0.1889 - acc: 0.9313 - val_loss: 0.3300 - val_acc: 0.8738\n",
            "Epoch 11/50\n",
            "2010/2010 [==============================] - 1s 285us/step - loss: 0.1793 - acc: 0.9338 - val_loss: 0.3072 - val_acc: 0.8825\n",
            "Epoch 12/50\n",
            "2010/2010 [==============================] - 1s 289us/step - loss: 0.1699 - acc: 0.9413 - val_loss: 0.3176 - val_acc: 0.9025\n",
            "Epoch 13/50\n",
            "2010/2010 [==============================] - 1s 278us/step - loss: 0.1773 - acc: 0.9378 - val_loss: 0.2950 - val_acc: 0.8750\n",
            "Epoch 14/50\n",
            "2010/2010 [==============================] - 1s 276us/step - loss: 0.1817 - acc: 0.9343 - val_loss: 0.3452 - val_acc: 0.8575\n",
            "Epoch 15/50\n",
            "2010/2010 [==============================] - 1s 279us/step - loss: 0.1718 - acc: 0.9393 - val_loss: 0.3668 - val_acc: 0.8900\n",
            "Epoch 16/50\n",
            "2010/2010 [==============================] - 1s 280us/step - loss: 0.1433 - acc: 0.9498 - val_loss: 0.4416 - val_acc: 0.8825\n",
            "Epoch 17/50\n",
            "2010/2010 [==============================] - 1s 271us/step - loss: 0.1487 - acc: 0.9507 - val_loss: 0.3784 - val_acc: 0.8938\n",
            "Epoch 18/50\n",
            "2010/2010 [==============================] - 1s 285us/step - loss: 0.1260 - acc: 0.9622 - val_loss: 0.4094 - val_acc: 0.9000\n",
            "Epoch 19/50\n",
            "2010/2010 [==============================] - 1s 280us/step - loss: 0.1514 - acc: 0.9502 - val_loss: 0.3822 - val_acc: 0.8750\n",
            "Epoch 20/50\n",
            "2010/2010 [==============================] - 1s 277us/step - loss: 0.1278 - acc: 0.9587 - val_loss: 0.3781 - val_acc: 0.8875\n",
            "Epoch 21/50\n",
            "2010/2010 [==============================] - 1s 285us/step - loss: 0.1353 - acc: 0.9547 - val_loss: 0.4372 - val_acc: 0.8762\n",
            "Epoch 22/50\n",
            "2010/2010 [==============================] - 1s 277us/step - loss: 0.1242 - acc: 0.9622 - val_loss: 0.4041 - val_acc: 0.8950\n",
            "Epoch 23/50\n",
            "2010/2010 [==============================] - 1s 273us/step - loss: 0.1201 - acc: 0.9597 - val_loss: 0.4271 - val_acc: 0.9012\n",
            "Epoch 24/50\n",
            "2010/2010 [==============================] - 1s 296us/step - loss: 0.1309 - acc: 0.9587 - val_loss: 0.4280 - val_acc: 0.8962\n",
            "Epoch 25/50\n",
            "2010/2010 [==============================] - 1s 285us/step - loss: 0.1345 - acc: 0.9562 - val_loss: 0.4347 - val_acc: 0.8788\n",
            "Epoch 26/50\n",
            "2010/2010 [==============================] - 1s 284us/step - loss: 0.1277 - acc: 0.9602 - val_loss: 0.3986 - val_acc: 0.8950\n",
            "Epoch 27/50\n",
            "2010/2010 [==============================] - 1s 279us/step - loss: 0.1226 - acc: 0.9667 - val_loss: 0.5120 - val_acc: 0.9012\n",
            "Epoch 28/50\n",
            "2010/2010 [==============================] - 1s 273us/step - loss: 0.1181 - acc: 0.9632 - val_loss: 0.4511 - val_acc: 0.8712\n",
            "Epoch 29/50\n",
            "2010/2010 [==============================] - 1s 273us/step - loss: 0.1044 - acc: 0.9716 - val_loss: 0.4532 - val_acc: 0.8888\n",
            "Epoch 30/50\n",
            "2010/2010 [==============================] - 1s 274us/step - loss: 0.1048 - acc: 0.9667 - val_loss: 0.5057 - val_acc: 0.8812\n",
            "Epoch 31/50\n",
            "2010/2010 [==============================] - 1s 286us/step - loss: 0.1222 - acc: 0.9692 - val_loss: 0.5075 - val_acc: 0.8850\n",
            "Epoch 32/50\n",
            "2010/2010 [==============================] - 1s 295us/step - loss: 0.1192 - acc: 0.9632 - val_loss: 0.5061 - val_acc: 0.8900\n",
            "Epoch 33/50\n",
            "2010/2010 [==============================] - 1s 279us/step - loss: 0.1087 - acc: 0.9706 - val_loss: 0.5299 - val_acc: 0.9012\n",
            "Epoch 34/50\n",
            "2010/2010 [==============================] - 1s 277us/step - loss: 0.1086 - acc: 0.9682 - val_loss: 0.4835 - val_acc: 0.9038\n",
            "Epoch 35/50\n",
            "2010/2010 [==============================] - 1s 295us/step - loss: 0.1104 - acc: 0.9716 - val_loss: 0.5106 - val_acc: 0.9000\n",
            "Epoch 36/50\n",
            "2010/2010 [==============================] - 1s 275us/step - loss: 0.0989 - acc: 0.9731 - val_loss: 0.5415 - val_acc: 0.8900\n",
            "Epoch 37/50\n",
            "2010/2010 [==============================] - 1s 271us/step - loss: 0.0874 - acc: 0.9736 - val_loss: 0.5974 - val_acc: 0.8912\n",
            "Epoch 38/50\n",
            "2010/2010 [==============================] - 1s 274us/step - loss: 0.1201 - acc: 0.9677 - val_loss: 0.5251 - val_acc: 0.8862\n",
            "Epoch 39/50\n",
            "2010/2010 [==============================] - 1s 275us/step - loss: 0.1108 - acc: 0.9697 - val_loss: 0.5216 - val_acc: 0.8900\n",
            "Epoch 40/50\n",
            "2010/2010 [==============================] - 1s 272us/step - loss: 0.1058 - acc: 0.9726 - val_loss: 0.5542 - val_acc: 0.8912\n",
            "Epoch 41/50\n",
            "2010/2010 [==============================] - 1s 279us/step - loss: 0.1004 - acc: 0.9731 - val_loss: 0.5506 - val_acc: 0.8925\n",
            "Epoch 42/50\n",
            "2010/2010 [==============================] - 1s 278us/step - loss: 0.0821 - acc: 0.9781 - val_loss: 0.5685 - val_acc: 0.8862\n",
            "Epoch 43/50\n",
            "2010/2010 [==============================] - 1s 285us/step - loss: 0.0962 - acc: 0.9766 - val_loss: 0.5742 - val_acc: 0.8875\n",
            "Epoch 44/50\n",
            "2010/2010 [==============================] - 1s 288us/step - loss: 0.0913 - acc: 0.9756 - val_loss: 0.6854 - val_acc: 0.8775\n",
            "Epoch 45/50\n",
            "2010/2010 [==============================] - 1s 293us/step - loss: 0.0998 - acc: 0.9687 - val_loss: 0.6270 - val_acc: 0.8837\n",
            "Epoch 46/50\n",
            "2010/2010 [==============================] - 1s 298us/step - loss: 0.0866 - acc: 0.9771 - val_loss: 0.5589 - val_acc: 0.8888\n",
            "Epoch 47/50\n",
            "2010/2010 [==============================] - 1s 299us/step - loss: 0.1020 - acc: 0.9751 - val_loss: 0.6755 - val_acc: 0.8925\n",
            "Epoch 48/50\n",
            "2010/2010 [==============================] - 1s 286us/step - loss: 0.1011 - acc: 0.9721 - val_loss: 0.6304 - val_acc: 0.8938\n",
            "Epoch 49/50\n",
            "2010/2010 [==============================] - 1s 296us/step - loss: 0.0898 - acc: 0.9791 - val_loss: 0.7872 - val_acc: 0.8650\n",
            "Epoch 50/50\n",
            "2010/2010 [==============================] - 1s 316us/step - loss: 0.0781 - acc: 0.9781 - val_loss: 0.7161 - val_acc: 0.8925\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "RuF3v01IKgYq",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The new score is 89,25%, and we see that in average there is a small improvement compared to the previous model.\n"
      ]
    },
    {
      "metadata": {
        "id": "K2pTGX3b7sUT",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "I tried to apply the method to the whole dataset but my computing resources aren't sufficient. I expect it would have improved a lot the score since the model would have had much more data to learn from.\n",
        "\n"
      ]
    }
  ]
}